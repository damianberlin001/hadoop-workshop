version: '2.1'
services:
  zookeeper-1:
    image: confluentinc/cp-zookeeper:5.2.1
    container_name: zookeeper-1
    hostname: zookeeper-1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: always

  broker-1:
    image: confluentinc/cp-kafka:5.2.1
    container_name: broker-1
    hostname: broker-1
    depends_on:
      - zookeeper-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${DOCKER_HOST_IP}:9092'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 9994
    restart: always
      
  schema-registry:
    image: confluentinc/cp-schema-registry:5.2.1
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - zookeeper-1
      - broker-1
    ports:
      - "8089:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper-1:2181'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: 'GET,POST,PUT,OPTIONS'
    restart: always

  schema-registry-ui:
    image: landoop/schema-registry-ui
    container_name: schema-registry-ui
    depends_on:
      - broker-1
      - schema-registry
    ports:
      - "28002:8000"
    environment:
      SCHEMAREGISTRY_URL: 'http://${PUBLIC_IP}:8089'
    restart: always

  kafka-manager:
    image: trivadis/kafka-manager
    container_name: kafka-manager
    depends_on:
      - zookeeper-1
      - broker-1
    ports:
      - "29000:9000"
    environment:
      ZK_HOSTS: 'zookeeper-1:2181'
      APPLICATION_SECRET: 'letmein'
    restart: always

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.1-java8
    container_name: namenode
    hostname: namenode
    volumes:
      - ./container-volume/namenode:/hadoop/dfs/name
      - ./data-transfer:/tmp/data-transfer
    ports:
      - "50070:50070"
    environment:    
      - CLUSTER_NAME=test
    env_file:
      - ./conf/hadoop.env
    healthcheck:
      interval: 5s
      retries: 100 
    restart: always

  datanode-1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.1-java8
    hostname: datanode-1
    container_name: datanode-1
    volumes:
      - ./container-volume/datanode-1:/hadoop/dfs/data
    ports:
      - "50075:50075"
    env_file:
      - ./conf/hadoop.env
    environment:
      - SERVICE_PRECONDITION="namenode:50070"      
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      interval: 5s
      retries: 100 
    restart: always

  datanode_2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.1-java8
    container_name: datanode-2
    hostname: datanode-2
    volumes:
      - ./container-volume/datanode-2:/hadoop/dfs/data
    ports:
      - "50076:50075"
    env_file:
      - ./conf/hadoop.env
    environment:
      - SERVICE_PRECONDITION="namenode:50070"      
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      interval: 5s
      retries: 100 
    restart: always

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.1.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    restart: always
  
  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.1.1-java8
    container_name: nodemanager
    hostname: nodemanager
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    restart: always
  
  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.1.1-java8
    container_name: historyserver
    hostname: historyserver
    ports:
      - "8188:8188"
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    restart: always

  spark-master:
    image: bde2020/spark-master:2.4.0-hadoop3.1 
    container_name: spark-master
    hostname: spark-master
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
    env_file:
      - ./conf/hadoop-hive.env  
    environment:
      - SPARK_PUBLIC_DNS=${PUBLIC_IP}
      - INIT_DAEMON_STEP=setup_spark
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-worker-1:
    image: bde2020/spark-worker:2.4.0-hadoop3.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    env_file:
      - ./conf/hadoop-hive.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
 #     CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-worker-2:
    image: bde2020/spark-worker:2.4.0-hadoop3.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    env_file:
      - ./conf/hadoop-hive.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
 #     CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-history:
    image: bde2020/spark-worker:2.4.0-hadoop3.1
    command: /spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    container_name: spark-history
    hostname: spark-history
    expose:
      - 18080
    ports:
      - 18080:18080
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  livy:
    image: registry.gitlab.com/rychly-edu/docker/docker-livy
    container_name: livy
    hostname: livy
    environment:
      - LIVY_PORT=8998
      - MASTER=spark://spark-master:7077
      - DEPLOY_MODE=cluster
    ports:
      - "8998:8998"
    restart: always

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    hostname: hive-server
    ports:
      - "10000:10000"
      - "10002:10002"
    env_file:
      - ./conf/hadoop-hive.env
    environment:
      - "HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore"
      - "SERVICE_PRECONDITION=hive-metastore:9083"
    restart: always

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - "9083:9083"
    env_file:
      - ./conf/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      - "SERVICE_PRECONDITION=namenode:50070 datanode-1:50075 hive-metastore-postgresql:5432"
    restart: always
  
  hive-metastore-postgresql:
    container_name: hive-metastore-postgresql
    hostname: hive-metastore-postgresql
    image: bde2020/hive-metastore-postgresql:2.3.0
    restart: always

  hue:
    image: gethue/hue:4.4.0
    container_name: hue
    hostname: hue
    dns: 8.8.8.8
    ports:
      - "28888:8888"
    volumes:
      - ./conf/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    depends_on:
      - hue-postgres
    restart: always

  hue-postgres:
    image: postgres:10
    container_name: hue-postgres
    hostname: hue-postgres
    environment:
      POSTGRES_DB: hue
      POSTGRES_PASSWORD: hue
      POSTGRES_USER: hue
    restart: always

  solr:
    image: solr:8.0.0
    container_name: solr
    hostname: solr
    environment:
      - ZK_HOST=zookeeper-1:2181
#    volumes:
#      - data:/opt/solr/server/solr/mycores
    ports:
      - "8983:8983"
    restart: always

  zeppelin:
    image: trivadis/apache-zeppelin:0.8.1-hadoop-3.1-spark-2.4.0
    container_name: zeppelin
    hostname: zeppelin
    ports:
      - "38081:8080"    
      - "4040:4040"    
    environment:
#      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SPARK_MASTER: "spark://spark-master:7077"
      # set spark-master for Zeppelin interpreter
      MASTER: "spark://spark-master:7077"
      SPARK_DRIVER_HOST: ${PUBLIC_IP}
      SPARK_DRIVER_BINDADDRESS: "0.0.0.0"
#      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
#      SPARK_SUBMIT_OPTIONS: "--jars /opt/sansa-examples/jars/sansa-examples-spark.jar --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrator=net.sansa_stack.owl.spark.dataset.UnmodifiableCollectionKryoRegistrator"
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
      - ./zeppelin/interpreter-setting.json:/opt/zeppelin/interpreter/spark/interpreter-setting.json

    restart: always

  # make sure to get Spark 2.4.0 to fit with the version of spark master (tag 59b402ce701d)
  jupyter:
    image: jupyter/all-spark-notebook:59b402ce701d
    container_name: jupyter
    hostname: jupyter
    ports: 
      - "38888:8888"
    environment:
      JUPYTER_ENABLE_LAB: "true"
      JUPYTER_TOKEN: "abc123"
      GRANT_SUDO: "true"
    restart: always

  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    ports:
      - '9000:9000'
#    volumes:
#      - './minio/data/:/data'
#      - './minio/config:/root/.minio'
    environment:
      MINIO_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      MINIO_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    command: server /data
    restart: always


